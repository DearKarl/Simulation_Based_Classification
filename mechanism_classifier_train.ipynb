{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003e9fff-734e-4a58-b495-9d3036adde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from random import gauss\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import os\n",
    "os.makedirs('trained_model', exist_ok=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251ec120-65c2-4413-bfe0-72e38a565528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train shape: (4950000, 4)\n",
      "x2_train shape: (4950000, 21, 12)\n",
      "y_train shape: (4950000, 20)\n",
      "x1_val shape: (50000, 4)\n",
      "x2_val shape: (50000, 21, 12)\n",
      "y_val shape: (50000, 20)\n",
      "x1_test shape: (100000, 4)\n",
      "x2_test shape: (100000, 7, 12)\n",
      "y_test shape: (100000, 20)\n"
     ]
    }
   ],
   "source": [
    "def load_datasets(path='simulation_data/', string='M1_M20_train_val_test_set'):\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "    filenames = ['x1_train', 'x2_train', 'y_train', 'x1_val', 'x2_val', 'y_val', 'x1_test', 'x2_test', 'y_test']\n",
    "    data = []\n",
    "    for file_name in filenames:\n",
    "        file_path = path + file_name + '_' + string + '.pkl'\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data.append(pickle.load(f))\n",
    "    return data\n",
    "\n",
    "x1_train, x2_train, y_train, x1_val, x2_val, y_val, x1_test, x2_test, y_test = \\\n",
    "    load_datasets(path='simulation_data/', string='M1_M20_train_val_test_set')\n",
    "\n",
    "print(\"x1_train shape:\", x1_train.shape)\n",
    "print(\"x2_train shape:\", x2_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x1_val shape:\", x1_val.shape)\n",
    "print(\"x2_val shape:\", x2_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x1_test shape:\", x1_test.shape)\n",
    "print(\"x2_test shape:\", x2_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b15082-aa84-4c16-be65-f89119c8f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (4950000, 20)\n",
      "Format: one-hot encoding\n",
      "\n",
      "Unique mechanism labels in y_train: ['M01', 'M02', 'M03', 'M04', 'M05', 'M06', 'M07', 'M08', 'M09', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M19', 'M20']\n",
      "Sample 1 (index 2647414): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M11\n",
      "Sample 2 (index 1027977): One-hot: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M05\n",
      "Sample 3 (index 3993827): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], Mechanism Label: M17\n",
      "Sample 4 (index 2955156): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M12\n",
      "Sample 5 (index 319154): One-hot: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M02\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "y_val shape: (50000, 20)\n",
      "Format: one-hot encoding\n",
      "\n",
      "Unique mechanism labels in y_val: ['M01', 'M02', 'M03', 'M04', 'M05', 'M06', 'M07', 'M08', 'M09', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M19', 'M20']\n",
      "Sample 1 (index 33553): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], Mechanism Label: M14\n",
      "Sample 2 (index 9427): One-hot: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M04\n",
      "Sample 3 (index 199): One-hot: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M01\n",
      "Sample 4 (index 12447): One-hot: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M05\n",
      "Sample 5 (index 39489): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], Mechanism Label: M16\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "y_test shape: (100000, 20)\n",
      "Format: one-hot encoding\n",
      "\n",
      "Unique mechanism labels in y_test: ['M01', 'M02', 'M03', 'M04', 'M05', 'M06', 'M07', 'M08', 'M09', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M19', 'M20']\n",
      "Sample 1 (index 75721): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], Mechanism Label: M16\n",
      "Sample 2 (index 80184): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], Mechanism Label: M17\n",
      "Sample 3 (index 19864): One-hot: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Mechanism Label: M04\n",
      "Sample 4 (index 76699): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], Mechanism Label: M16\n",
      "Sample 5 (index 92991): One-hot: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], Mechanism Label: M19\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "The label for each sample is the corresponding mechanism used to generate its kinetic data, encoded as a one-hot vector as follows:\n",
      "\n",
      "M 1: (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 2: (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 3: (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 4: (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 5: (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 6: (0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 7: (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 8: (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M 9: (0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M10: (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M11: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M12: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "M13: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)\n",
      "M14: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0)\n",
      "M15: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0)\n",
      "M16: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0)\n",
      "M17: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n",
      "M18: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0)\n",
      "M19: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0)\n",
      "M20: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "def check_one_hot(arr):\n",
    "    if arr.ndim != 2 or arr.shape[1] != 20:\n",
    "        return False\n",
    "    row_sums = np.sum(arr, axis=1)\n",
    "    return np.all((row_sums == 1) & (np.count_nonzero(arr, axis=1) == 1))\n",
    "\n",
    "def print_label_info(arr, name, n_show=5, seed=42):\n",
    "    print(f\"{name} shape: {arr.shape}\")\n",
    "    if check_one_hot(arr):\n",
    "        print(\"Format: one-hot encoding\\n\")\n",
    "        unique_indices = np.unique(np.argmax(arr, axis=1))\n",
    "        print(f\"Unique mechanism labels in {name}: {[f'M{idx+1:02d}' for idx in unique_indices]}\")\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(arr.shape[0], n_show, replace=False)\n",
    "        for i, idx in enumerate(indices):\n",
    "            label = np.argmax(arr[idx])\n",
    "            one_hot_str = ', '.join(str(int(x)) for x in arr[idx])\n",
    "            print(f\"Sample {i+1} (index {idx}): One-hot: [{one_hot_str}], Mechanism Label: M{label+1:02d}\")\n",
    "    else:\n",
    "        print(\"Format: NOT one-hot encoding\")\n",
    "    print()\n",
    "    print('-'*80)\n",
    "\n",
    "print_label_info(y_train, \"y_train\")\n",
    "print_label_info(y_val, \"y_val\")\n",
    "print_label_info(y_test, \"y_test\")\n",
    "\n",
    "print(\"The label for each sample is the corresponding mechanism used to generate its kinetic data, encoded as a one-hot vector as follows:\\n\")\n",
    "for i in range(20):\n",
    "    vec = ['0']*20\n",
    "    vec[i] = '1'\n",
    "    print(f\"M{i+1:>2}: ({', '.join(vec)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107c0a63-6709-4171-abc2-73eebf5dee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_classifier(input1_shape, input2_shape, output_shape, dropout_rate=0.3):\n",
    "    initial_concentrations = Input(shape=input1_shape)\n",
    "    kinetics = Input(shape=(None, input2_shape[-1]))\n",
    "\n",
    "    c = Dense(200, activation='relu')(initial_concentrations)\n",
    "    c = Dropout(dropout_rate)(c)\n",
    "\n",
    "    k = LSTM(200, return_sequences=True, dropout=dropout_rate)(kinetics)\n",
    "    k = LSTM(200, dropout=dropout_rate)(k)\n",
    "    k = Dropout(dropout_rate)(k)\n",
    "\n",
    "    combined = concatenate([c, k])\n",
    "    pred = Dense(200, activation='relu', kernel_initializer='he_uniform')(combined)\n",
    "    pred = Dropout(dropout_rate)(pred)\n",
    "    pred = Dense(output_shape[1], activation='softmax', name='Dense_5')(pred)\n",
    "\n",
    "    model = Model(inputs=[initial_concentrations, kinetics], outputs=pred)\n",
    "    return model\n",
    "\n",
    "class KineticsBatchGenerator(Sequence):\n",
    "    def __init__(self, X, y, tps, error_range, seed_value=1, batch_size=1024, shuffle=True):\n",
    "        self.x1, self.x2 = X\n",
    "        self.y = y\n",
    "        self.tps = tps\n",
    "        self.error_range = error_range\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.total_batches = self.__len__()\n",
    "        self.n_runs = 4\n",
    "        self.species = [1, 2]\n",
    "        self.error_species = len(self.species)\n",
    "        self.error_dict = {key: [] for key in error_range}\n",
    "        np.random.seed(seed_value)\n",
    "        self.randomstate = np.random.default_rng(seed_value)\n",
    "        self.on_epoch_end()\n",
    "        examples, timepoints, curves = self.x2.shape\n",
    "        columns = curves // self.n_runs\n",
    "        self.index_species = []\n",
    "        for i in range(self.n_runs):\n",
    "            t_species = [a + (i * columns) for a in self.species]\n",
    "            self.index_species = self.index_species + t_species\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__data_generation(index)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.y))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        for error in self.error_range:\n",
    "            if error != 0:\n",
    "                preu = np.array([gauss(0, error / 100) for _ in range(self.batch_size * np.max(self.tps) * self.error_species * self.n_runs)])\n",
    "                self.error_dict[error] = np.reshape(preu, (self.batch_size, np.max(self.tps), self.error_species * self.n_runs))\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        x1 = self.x1[self.batch_size * index: self.batch_size * (index + 1)]\n",
    "        x2 = self.introduce_error(self.trim_x2(self.x2[self.batch_size * index: self.batch_size * (index + 1)], self.tps, index))\n",
    "        y = self.y[self.batch_size * index: self.batch_size * (index + 1)]\n",
    "        x = [x1, x2]\n",
    "        return x, y\n",
    "\n",
    "    def trim_x2(self, original_x2, tps, index):\n",
    "        original_tps = original_x2.shape[1]\n",
    "        n_tps = self.randomstate.choice(tps)\n",
    "        idx = np.sort(np.append(self.randomstate.choice(original_tps - 1, n_tps, replace=False) + 1, [0]))\n",
    "        return original_x2[:, idx].copy()\n",
    "\n",
    "    def introduce_error(self, x2):\n",
    "        examples, timepoints, curves = x2.shape\n",
    "        error = self.randomstate.choice(self.error_range)\n",
    "        if error != 0:\n",
    "            x2[:, 1:, self.index_species] = x2[:, 1:, self.index_species] + self.error_dict[error][:, 0:timepoints - 1]\n",
    "        return x2\n",
    "\n",
    "def get_top_mechanism_indices(predictions):\n",
    "    threshold = 0.99\n",
    "    index = np.argsort(predictions)\n",
    "    prob = 0\n",
    "    grouping = []\n",
    "    for j in index[::-1]:\n",
    "        prob += predictions[j]\n",
    "        grouping.append(j)\n",
    "        if prob >= threshold:\n",
    "            break\n",
    "    return grouping\n",
    "\n",
    "def mechanism_indices_to_names(indices):\n",
    "    mechanism_list = ['M' + str(i) for i in range(1, 21)]\n",
    "    grouping_names = [mechanism_list[i] for i in indices]\n",
    "    return grouping_names\n",
    "\n",
    "def plot_kinetic_profiles_and_mechanisms(x_list, mechanism_names, columns=3, s=30, A0=1, show=True, overlay=False, labels=None):\n",
    "    x1, x2 = x_list\n",
    "    rows = len(x1)\n",
    "    curves = x2.shape[2] // columns\n",
    "    if not overlay:\n",
    "        plt.figure(figsize=(4 * columns, 7 * rows))\n",
    "    i = 1\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            plt.subplot(2, columns, i)\n",
    "            if not overlay:\n",
    "                if column == 3:\n",
    "                    title = r'[Cat]$_{0}$ = ' + str(round(x1[row, column], 5))\n",
    "                else:\n",
    "                    percentage = ' (' + str(round(x1[row, column] / A0 * 100, 3)) + ' mol%)'\n",
    "                    title = r'[Cat]$_{0}$ = ' + str(round(x1[row, column], 5)) + percentage\n",
    "                plt.title(title)\n",
    "            if type(labels) == list:\n",
    "                label = labels\n",
    "            else:\n",
    "                label = None\n",
    "            for j in range(1, curves):\n",
    "                if type(labels) == list:\n",
    "                    label = labels[j - 1]\n",
    "                else:\n",
    "                    label = None\n",
    "                plt.scatter(x2[row, :, column * curves], x2[row, :, (j) + column * curves], s, label=label)\n",
    "            if not overlay:\n",
    "                plt.xlim(left=0, right=max(x2[row, :, column * curves]) * 1.1)\n",
    "                plt.ylim(bottom=0)\n",
    "            i += 1\n",
    "    plt.tight_layout()\n",
    "    mechs = []\n",
    "    for i, name in enumerate(mechanism_names):\n",
    "        mechs.append(img.imread('Images/' + name + '.jpg'))\n",
    "    length = len(mechs)\n",
    "    for i, mech in enumerate(mechs):\n",
    "        plt.subplot(2, length, length + i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(mech)\n",
    "\n",
    "MODEL_COLUMN_MAP = {\n",
    "    'M1_20_model_bayes': ['Time', 'S', 'P', 'catT']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aad58-bdc2-479b-b54e-91222e65f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.7820 - categorical_accuracy: 0.0934\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.12746, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 499s 206ms/step - loss: 2.7820 - categorical_accuracy: 0.0934 - val_loss: 2.6131 - val_categorical_accuracy: 0.1275\n",
      "Epoch 2/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.6218 - categorical_accuracy: 0.1219\n",
      "Epoch 2: val_categorical_accuracy improved from 0.12746 to 0.14226, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 507s 210ms/step - loss: 2.6218 - categorical_accuracy: 0.1219 - val_loss: 2.4725 - val_categorical_accuracy: 0.1423\n",
      "Epoch 3/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.5730 - categorical_accuracy: 0.1317\n",
      "Epoch 3: val_categorical_accuracy improved from 0.14226 to 0.16968, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 510s 211ms/step - loss: 2.5730 - categorical_accuracy: 0.1317 - val_loss: 2.4464 - val_categorical_accuracy: 0.1697\n",
      "Epoch 4/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.5435 - categorical_accuracy: 0.1341\n",
      "Epoch 4: val_categorical_accuracy improved from 0.16968 to 0.19150, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 506s 210ms/step - loss: 2.5435 - categorical_accuracy: 0.1341 - val_loss: 2.4045 - val_categorical_accuracy: 0.1915\n",
      "Epoch 5/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.5095 - categorical_accuracy: 0.1399\n",
      "Epoch 5: val_categorical_accuracy improved from 0.19150 to 0.20266, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 518s 215ms/step - loss: 2.5095 - categorical_accuracy: 0.1399 - val_loss: 2.3388 - val_categorical_accuracy: 0.2027\n",
      "Epoch 6/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.4692 - categorical_accuracy: 0.1511\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.20266\n",
      "2416/2416 [==============================] - 513s 212ms/step - loss: 2.4692 - categorical_accuracy: 0.1511 - val_loss: 2.3183 - val_categorical_accuracy: 0.1907\n",
      "Epoch 7/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.4352 - categorical_accuracy: 0.1578\n",
      "Epoch 7: val_categorical_accuracy improved from 0.20266 to 0.22782, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 513s 212ms/step - loss: 2.4352 - categorical_accuracy: 0.1578 - val_loss: 2.2477 - val_categorical_accuracy: 0.2278\n",
      "Epoch 8/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.4102 - categorical_accuracy: 0.1675\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.22782\n",
      "2416/2416 [==============================] - 515s 213ms/step - loss: 2.4102 - categorical_accuracy: 0.1675 - val_loss: 2.2254 - val_categorical_accuracy: 0.2249\n",
      "Epoch 9/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.3810 - categorical_accuracy: 0.1766\n",
      "Epoch 9: val_categorical_accuracy improved from 0.22782 to 0.24780, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 508s 210ms/step - loss: 2.3810 - categorical_accuracy: 0.1766 - val_loss: 2.2031 - val_categorical_accuracy: 0.2478\n",
      "Epoch 10/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.3609 - categorical_accuracy: 0.1757\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.24780\n",
      "2416/2416 [==============================] - 517s 214ms/step - loss: 2.3609 - categorical_accuracy: 0.1757 - val_loss: 2.1787 - val_categorical_accuracy: 0.2426\n",
      "Epoch 11/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.3415 - categorical_accuracy: 0.1841\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.24780\n",
      "2416/2416 [==============================] - 510s 211ms/step - loss: 2.3415 - categorical_accuracy: 0.1841 - val_loss: 2.1722 - val_categorical_accuracy: 0.2311\n",
      "Epoch 12/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.3266 - categorical_accuracy: 0.1880\n",
      "Epoch 12: val_categorical_accuracy improved from 0.24780 to 0.25898, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 510s 211ms/step - loss: 2.3266 - categorical_accuracy: 0.1880 - val_loss: 2.1245 - val_categorical_accuracy: 0.2590\n",
      "Epoch 13/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.3109 - categorical_accuracy: 0.1921\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.25898\n",
      "2416/2416 [==============================] - 516s 213ms/step - loss: 2.3109 - categorical_accuracy: 0.1921 - val_loss: 2.1367 - val_categorical_accuracy: 0.2445\n",
      "Epoch 14/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2935 - categorical_accuracy: 0.1953\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.25898\n",
      "2416/2416 [==============================] - 520s 215ms/step - loss: 2.2935 - categorical_accuracy: 0.1953 - val_loss: 2.1377 - val_categorical_accuracy: 0.2439\n",
      "Epoch 15/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2823 - categorical_accuracy: 0.1996\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.25898\n",
      "2416/2416 [==============================] - 512s 212ms/step - loss: 2.2823 - categorical_accuracy: 0.1996 - val_loss: 2.1151 - val_categorical_accuracy: 0.2516\n",
      "Epoch 16/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2806 - categorical_accuracy: 0.1988\n",
      "Epoch 16: val_categorical_accuracy improved from 0.25898 to 0.26358, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 520s 215ms/step - loss: 2.2806 - categorical_accuracy: 0.1988 - val_loss: 2.0901 - val_categorical_accuracy: 0.2636\n",
      "Epoch 17/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2618 - categorical_accuracy: 0.2056\n",
      "Epoch 17: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 522s 216ms/step - loss: 2.2618 - categorical_accuracy: 0.2056 - val_loss: 2.1263 - val_categorical_accuracy: 0.2535\n",
      "Epoch 18/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2502 - categorical_accuracy: 0.2080\n",
      "Epoch 18: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 526s 218ms/step - loss: 2.2502 - categorical_accuracy: 0.2080 - val_loss: 2.0699 - val_categorical_accuracy: 0.2525\n",
      "Epoch 19/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2455 - categorical_accuracy: 0.2093\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 523s 216ms/step - loss: 2.2455 - categorical_accuracy: 0.2093 - val_loss: 2.1152 - val_categorical_accuracy: 0.2509\n",
      "Epoch 20/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2314 - categorical_accuracy: 0.2157\n",
      "Epoch 20: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 523s 217ms/step - loss: 2.2314 - categorical_accuracy: 0.2157 - val_loss: 2.0765 - val_categorical_accuracy: 0.2466\n",
      "Epoch 21/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2342 - categorical_accuracy: 0.2112\n",
      "Epoch 21: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 525s 217ms/step - loss: 2.2342 - categorical_accuracy: 0.2112 - val_loss: 2.0718 - val_categorical_accuracy: 0.2587\n",
      "Epoch 22/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2181 - categorical_accuracy: 0.2204\n",
      "Epoch 22: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 521s 216ms/step - loss: 2.2181 - categorical_accuracy: 0.2204 - val_loss: 2.0744 - val_categorical_accuracy: 0.2529\n",
      "Epoch 23/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2142 - categorical_accuracy: 0.2212\n",
      "Epoch 23: val_categorical_accuracy did not improve from 0.26358\n",
      "2416/2416 [==============================] - 530s 219ms/step - loss: 2.2142 - categorical_accuracy: 0.2212 - val_loss: 2.1746 - val_categorical_accuracy: 0.2377\n",
      "Epoch 24/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2132 - categorical_accuracy: 0.2192\n",
      "Epoch 24: val_categorical_accuracy improved from 0.26358 to 0.29752, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 515s 213ms/step - loss: 2.2132 - categorical_accuracy: 0.2192 - val_loss: 2.0152 - val_categorical_accuracy: 0.2975\n",
      "Epoch 25/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.2012 - categorical_accuracy: 0.2257\n",
      "Epoch 25: val_categorical_accuracy did not improve from 0.29752\n",
      "2416/2416 [==============================] - 509s 211ms/step - loss: 2.2012 - categorical_accuracy: 0.2257 - val_loss: 1.9953 - val_categorical_accuracy: 0.2866\n",
      "Epoch 26/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1893 - categorical_accuracy: 0.2269\n",
      "Epoch 26: val_categorical_accuracy did not improve from 0.29752\n",
      "2416/2416 [==============================] - 524s 217ms/step - loss: 2.1893 - categorical_accuracy: 0.2269 - val_loss: 2.1071 - val_categorical_accuracy: 0.2414\n",
      "Epoch 27/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1861 - categorical_accuracy: 0.2266\n",
      "Epoch 27: val_categorical_accuracy did not improve from 0.29752\n",
      "2416/2416 [==============================] - 527s 218ms/step - loss: 2.1861 - categorical_accuracy: 0.2266 - val_loss: 2.1970 - val_categorical_accuracy: 0.2143\n",
      "Epoch 28/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1691 - categorical_accuracy: 0.2337\n",
      "Epoch 28: val_categorical_accuracy improved from 0.29752 to 0.30566, saving model to trained_model/best_model_weights\n",
      "2416/2416 [==============================] - 524s 217ms/step - loss: 2.1691 - categorical_accuracy: 0.2337 - val_loss: 1.9767 - val_categorical_accuracy: 0.3057\n",
      "Epoch 29/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1655 - categorical_accuracy: 0.2334\n",
      "Epoch 29: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 521s 216ms/step - loss: 2.1655 - categorical_accuracy: 0.2334 - val_loss: 2.0755 - val_categorical_accuracy: 0.2654\n",
      "Epoch 30/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1512 - categorical_accuracy: 0.2366\n",
      "Epoch 30: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 521s 216ms/step - loss: 2.1512 - categorical_accuracy: 0.2366 - val_loss: 2.1489 - val_categorical_accuracy: 0.2321\n",
      "Epoch 31/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1519 - categorical_accuracy: 0.2382\n",
      "Epoch 31: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 522s 216ms/step - loss: 2.1519 - categorical_accuracy: 0.2382 - val_loss: 2.0472 - val_categorical_accuracy: 0.2753\n",
      "Epoch 32/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1521 - categorical_accuracy: 0.2370\n",
      "Epoch 32: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 532s 220ms/step - loss: 2.1521 - categorical_accuracy: 0.2370 - val_loss: 2.0748 - val_categorical_accuracy: 0.2571\n",
      "Epoch 33/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1584 - categorical_accuracy: 0.2380\n",
      "Epoch 33: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 521s 216ms/step - loss: 2.1584 - categorical_accuracy: 0.2380 - val_loss: 2.1939 - val_categorical_accuracy: 0.2012\n",
      "Epoch 34/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1358 - categorical_accuracy: 0.2411\n",
      "Epoch 34: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 507s 210ms/step - loss: 2.1358 - categorical_accuracy: 0.2411 - val_loss: 2.2498 - val_categorical_accuracy: 0.1973\n",
      "Epoch 35/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1362 - categorical_accuracy: 0.2392\n",
      "Epoch 35: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 516s 213ms/step - loss: 2.1362 - categorical_accuracy: 0.2392 - val_loss: 2.2825 - val_categorical_accuracy: 0.1981\n",
      "Epoch 36/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1065 - categorical_accuracy: 0.2519\n",
      "Epoch 36: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 536s 222ms/step - loss: 2.1065 - categorical_accuracy: 0.2519 - val_loss: 2.1413 - val_categorical_accuracy: 0.2510\n",
      "Epoch 37/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1235 - categorical_accuracy: 0.2463\n",
      "Epoch 37: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 514s 213ms/step - loss: 2.1235 - categorical_accuracy: 0.2463 - val_loss: 2.4831 - val_categorical_accuracy: 0.1880\n",
      "Epoch 38/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.1045 - categorical_accuracy: 0.2476\n",
      "Epoch 38: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 523s 217ms/step - loss: 2.1045 - categorical_accuracy: 0.2476 - val_loss: 2.4178 - val_categorical_accuracy: 0.1716\n",
      "Epoch 39/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0895 - categorical_accuracy: 0.2558\n",
      "Epoch 39: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 531s 220ms/step - loss: 2.0895 - categorical_accuracy: 0.2558 - val_loss: 2.1647 - val_categorical_accuracy: 0.2324\n",
      "Epoch 40/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0893 - categorical_accuracy: 0.2551\n",
      "Epoch 40: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 531s 220ms/step - loss: 2.0893 - categorical_accuracy: 0.2551 - val_loss: 2.4731 - val_categorical_accuracy: 0.1919\n",
      "Epoch 41/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0789 - categorical_accuracy: 0.2576\n",
      "Epoch 41: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 525s 217ms/step - loss: 2.0789 - categorical_accuracy: 0.2576 - val_loss: 2.2593 - val_categorical_accuracy: 0.2319\n",
      "Epoch 42/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0637 - categorical_accuracy: 0.2626\n",
      "Epoch 42: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 528s 218ms/step - loss: 2.0637 - categorical_accuracy: 0.2626 - val_loss: 2.3906 - val_categorical_accuracy: 0.2218\n",
      "Epoch 43/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0563 - categorical_accuracy: 0.2634\n",
      "Epoch 43: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 528s 219ms/step - loss: 2.0563 - categorical_accuracy: 0.2634 - val_loss: 2.5431 - val_categorical_accuracy: 0.1832\n",
      "Epoch 44/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0473 - categorical_accuracy: 0.2653\n",
      "Epoch 44: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 516s 214ms/step - loss: 2.0473 - categorical_accuracy: 0.2653 - val_loss: 2.5524 - val_categorical_accuracy: 0.1830\n",
      "Epoch 45/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0350 - categorical_accuracy: 0.2704\n",
      "Epoch 45: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 529s 219ms/step - loss: 2.0350 - categorical_accuracy: 0.2704 - val_loss: 2.4604 - val_categorical_accuracy: 0.2192\n",
      "Epoch 46/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0352 - categorical_accuracy: 0.2672\n",
      "Epoch 46: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 529s 219ms/step - loss: 2.0352 - categorical_accuracy: 0.2672 - val_loss: 2.6366 - val_categorical_accuracy: 0.1871\n",
      "Epoch 47/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0173 - categorical_accuracy: 0.2716\n",
      "Epoch 47: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 528s 218ms/step - loss: 2.0173 - categorical_accuracy: 0.2716 - val_loss: 2.5480 - val_categorical_accuracy: 0.2077\n",
      "Epoch 48/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0133 - categorical_accuracy: 0.2754\n",
      "Epoch 48: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 523s 217ms/step - loss: 2.0133 - categorical_accuracy: 0.2754 - val_loss: 2.5461 - val_categorical_accuracy: 0.1980\n",
      "Epoch 49/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0117 - categorical_accuracy: 0.2731\n",
      "Epoch 49: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 524s 217ms/step - loss: 2.0117 - categorical_accuracy: 0.2731 - val_loss: 2.5158 - val_categorical_accuracy: 0.2151\n",
      "Epoch 50/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 2.0003 - categorical_accuracy: 0.2789\n",
      "Epoch 50: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 528s 218ms/step - loss: 2.0003 - categorical_accuracy: 0.2789 - val_loss: 2.6657 - val_categorical_accuracy: 0.1932\n",
      "Epoch 51/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9974 - categorical_accuracy: 0.2798\n",
      "Epoch 51: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 525s 217ms/step - loss: 1.9974 - categorical_accuracy: 0.2798 - val_loss: 2.6097 - val_categorical_accuracy: 0.2060\n",
      "Epoch 52/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9830 - categorical_accuracy: 0.2812\n",
      "Epoch 52: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 522s 216ms/step - loss: 1.9830 - categorical_accuracy: 0.2812 - val_loss: 2.7807 - val_categorical_accuracy: 0.1702\n",
      "Epoch 53/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9726 - categorical_accuracy: 0.2863\n",
      "Epoch 53: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 538s 223ms/step - loss: 1.9726 - categorical_accuracy: 0.2863 - val_loss: 2.6213 - val_categorical_accuracy: 0.2069\n",
      "Epoch 54/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9709 - categorical_accuracy: 0.2872\n",
      "Epoch 54: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 523s 217ms/step - loss: 1.9709 - categorical_accuracy: 0.2872 - val_loss: 2.6556 - val_categorical_accuracy: 0.2120\n",
      "Epoch 55/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9543 - categorical_accuracy: 0.2929\n",
      "Epoch 55: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 527s 218ms/step - loss: 1.9543 - categorical_accuracy: 0.2929 - val_loss: 2.5428 - val_categorical_accuracy: 0.2307\n",
      "Epoch 56/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9483 - categorical_accuracy: 0.2896\n",
      "Epoch 56: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 518s 214ms/step - loss: 1.9483 - categorical_accuracy: 0.2896 - val_loss: 2.7453 - val_categorical_accuracy: 0.1984\n",
      "Epoch 57/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9617 - categorical_accuracy: 0.2895\n",
      "Epoch 57: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 521s 216ms/step - loss: 1.9617 - categorical_accuracy: 0.2895 - val_loss: 2.6424 - val_categorical_accuracy: 0.2202\n",
      "Epoch 58/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9332 - categorical_accuracy: 0.2961\n",
      "Epoch 58: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 525s 217ms/step - loss: 1.9332 - categorical_accuracy: 0.2961 - val_loss: 2.7798 - val_categorical_accuracy: 0.1959\n",
      "Epoch 59/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9366 - categorical_accuracy: 0.2964\n",
      "Epoch 59: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 513s 212ms/step - loss: 1.9366 - categorical_accuracy: 0.2964 - val_loss: 2.9406 - val_categorical_accuracy: 0.1790\n",
      "Epoch 60/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9398 - categorical_accuracy: 0.2944\n",
      "Epoch 60: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 516s 214ms/step - loss: 1.9398 - categorical_accuracy: 0.2944 - val_loss: 2.7042 - val_categorical_accuracy: 0.2148\n",
      "Epoch 61/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9330 - categorical_accuracy: 0.2940\n",
      "Epoch 61: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 513s 212ms/step - loss: 1.9330 - categorical_accuracy: 0.2940 - val_loss: 3.2207 - val_categorical_accuracy: 0.1553\n",
      "Epoch 62/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9338 - categorical_accuracy: 0.2968\n",
      "Epoch 62: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 515s 213ms/step - loss: 1.9338 - categorical_accuracy: 0.2968 - val_loss: 2.6526 - val_categorical_accuracy: 0.1982\n",
      "Epoch 63/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9299 - categorical_accuracy: 0.2989\n",
      "Epoch 63: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 513s 212ms/step - loss: 1.9299 - categorical_accuracy: 0.2989 - val_loss: 3.0031 - val_categorical_accuracy: 0.1889\n",
      "Epoch 64/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9187 - categorical_accuracy: 0.3008\n",
      "Epoch 64: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 509s 211ms/step - loss: 1.9187 - categorical_accuracy: 0.3008 - val_loss: 2.6851 - val_categorical_accuracy: 0.1902\n",
      "Epoch 65/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9061 - categorical_accuracy: 0.3030\n",
      "Epoch 65: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 500s 207ms/step - loss: 1.9061 - categorical_accuracy: 0.3030 - val_loss: 2.7218 - val_categorical_accuracy: 0.2101\n",
      "Epoch 66/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.8919 - categorical_accuracy: 0.3111\n",
      "Epoch 66: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 526s 218ms/step - loss: 1.8919 - categorical_accuracy: 0.3111 - val_loss: 2.8400 - val_categorical_accuracy: 0.1880\n",
      "Epoch 67/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.9040 - categorical_accuracy: 0.3027\n",
      "Epoch 67: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 529s 219ms/step - loss: 1.9040 - categorical_accuracy: 0.3027 - val_loss: 2.7832 - val_categorical_accuracy: 0.2099\n",
      "Epoch 68/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.8919 - categorical_accuracy: 0.3110\n",
      "Epoch 68: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 514s 213ms/step - loss: 1.8919 - categorical_accuracy: 0.3110 - val_loss: 2.7385 - val_categorical_accuracy: 0.1977\n",
      "Epoch 69/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.8964 - categorical_accuracy: 0.3079\n",
      "Epoch 69: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 523s 216ms/step - loss: 1.8964 - categorical_accuracy: 0.3079 - val_loss: 2.6539 - val_categorical_accuracy: 0.2056\n",
      "Epoch 70/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.8813 - categorical_accuracy: 0.3106\n",
      "Epoch 70: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 518s 214ms/step - loss: 1.8813 - categorical_accuracy: 0.3106 - val_loss: 2.8712 - val_categorical_accuracy: 0.1961\n",
      "Epoch 71/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.8808 - categorical_accuracy: 0.3132\n",
      "Epoch 71: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 512s 212ms/step - loss: 1.8808 - categorical_accuracy: 0.3132 - val_loss: 2.7349 - val_categorical_accuracy: 0.1951\n",
      "Epoch 72/3000\n",
      "2416/2416 [==============================] - ETA: 0s - loss: 1.8684 - categorical_accuracy: 0.3137\n",
      "Epoch 72: val_categorical_accuracy did not improve from 0.30566\n",
      "2416/2416 [==============================] - 526s 218ms/step - loss: 1.8684 - categorical_accuracy: 0.3137 - val_loss: 2.7990 - val_categorical_accuracy: 0.1972\n",
      "Epoch 73/3000\n",
      "1870/2416 [======================>.......] - ETA: 1:53 - loss: 1.8894 - categorical_accuracy: 0.3081"
     ]
    }
   ],
   "source": [
    "def train_mechanism_model(\n",
    "    tps = [3,4,5,6,7,8,9,10,15,20],\n",
    "    error_range = [0,0.5,1,2],\n",
    "    epochs = 3000,\n",
    "    start = 0,\n",
    "    batch_size_training = 2048\n",
    "):\n",
    "    x1_train, x2_train, y_train, x1_val, x2_val, y_val, x1_test, x2_test, y_test = load_datasets()\n",
    "\n",
    "    model = build_lstm_classifier(\n",
    "        input1_shape = x1_train.shape[1:],\n",
    "        input2_shape = x2_train.shape[1:],\n",
    "        output_shape = y_train.shape  \n",
    "    )\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        patience=300,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"trained_model/best_model_weights\",\n",
    "        monitor='val_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        KineticsBatchGenerator([x1_train, x2_train], y_train, tps, error_range, batch_size=batch_size_training),\n",
    "        validation_data=KineticsBatchGenerator([x1_val, x2_val], y_val, tps, error_range, batch_size=25),\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        epochs=start+epochs,\n",
    "        initial_epoch=start\n",
    "    )\n",
    "\n",
    "    model.save('trained_model/M1_20_model_bayes')\n",
    "    print('Training completed. Optimal weights saved to: trained_model/best_model_weights')\n",
    "    print('Trained model saved to: trained_model/M1_20_model_bayes')\n",
    "    return model\n",
    "\n",
    "model = train_mechanism_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fe2c4-693e-48d1-b5ad-d9f9b94fb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, x_inputs, n_mc=100):\n",
    "    preds = [model(x_inputs, training=True).numpy() for _ in range(n_mc)]\n",
    "    all_probs = np.stack(preds, axis=0)\n",
    "    mean_probs = all_probs.mean(axis=0)\n",
    "    std_probs = all_probs.std(axis=0)\n",
    "    return mean_probs, std_probs, all_probs\n",
    "\n",
    "def credible_set(probs, threshold=0.95):\n",
    "    idx = np.argsort(probs)[::-1]\n",
    "    cumsum = np.cumsum(probs[idx])\n",
    "    k = np.searchsorted(cumsum, threshold) + 1\n",
    "    return idx[:k]\n",
    "\n",
    "def mechanism_indices_to_names(indices):\n",
    "    mechanism_list = ['M' + str(i) for i in range(1, 21)]\n",
    "    return [mechanism_list[i] for i in indices]\n",
    "\n",
    "model = load_model('trained_model/M1_20_model_bayes', compile=False)\n",
    "model.load_weights('trained_model/best_model_weights')\n",
    "\n",
    "mean_probs, std_probs, all_probs = mc_dropout_predict(model, [x1_test, x2_test], n_mc=100)\n",
    "entropies = entropy(mean_probs.T).T\n",
    "credible_sets = [credible_set(p, 0.95) for p in mean_probs]\n",
    "\n",
    "np.savez('bayesian_results_mc_dropout.npz',\n",
    "         mean_probs=mean_probs,\n",
    "         std_probs=std_probs,\n",
    "         entropies=entropies,\n",
    "         credible_sets=credible_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi-mac-env",
   "language": "python",
   "name": "sbi-mac-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
