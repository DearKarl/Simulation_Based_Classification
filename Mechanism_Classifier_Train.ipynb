{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e9fff-734e-4a58-b495-9d3036adde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from random import gauss\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import os\n",
    "os.makedirs('trained_model', exist_ok=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ec120-65c2-4413-bfe0-72e38a565528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(path='simulation_data/', string='M1_M20_train_val_test_set'):\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "    filenames = ['x1_train', 'x2_train', 'y_train', 'x1_val', 'x2_val', 'y_val', 'x1_test', 'x2_test', 'y_test']\n",
    "    data = []\n",
    "    for file_name in filenames:\n",
    "        file_path = path + file_name + '_' + string + '.pkl'\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data.append(pickle.load(f))\n",
    "    return data\n",
    "\n",
    "x1_train, x2_train, y_train, x1_val, x2_val, y_val, x1_test, x2_test, y_test = \\\n",
    "    load_datasets(path='simulation_data/', string='M1_M20_train_val_test_set')\n",
    "\n",
    "print(\"x1_train shape:\", x1_train.shape)\n",
    "print(\"x2_train shape:\", x2_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x1_val shape:\", x1_val.shape)\n",
    "print(\"x2_val shape:\", x2_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x1_test shape:\", x1_test.shape)\n",
    "print(\"x2_test shape:\", x2_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b15082-aa84-4c16-be65-f89119c8f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_one_hot(arr):\n",
    "    if arr.ndim != 2 or arr.shape[1] != 20:\n",
    "        return False\n",
    "    row_sums = np.sum(arr, axis=1)\n",
    "    return np.all((row_sums == 1) & (np.count_nonzero(arr, axis=1) == 1))\n",
    "\n",
    "def print_label_info(arr, name, n_show=5, seed=42):\n",
    "    print(f\"{name} shape: {arr.shape}\")\n",
    "    if check_one_hot(arr):\n",
    "        print(\"Format: one-hot encoding\\n\")\n",
    "        unique_indices = np.unique(np.argmax(arr, axis=1))\n",
    "        print(f\"Unique mechanism labels in {name}: {[f'M{idx+1:02d}' for idx in unique_indices]}\")\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(arr.shape[0], n_show, replace=False)\n",
    "        for i, idx in enumerate(indices):\n",
    "            label = np.argmax(arr[idx])\n",
    "            one_hot_str = ', '.join(str(int(x)) for x in arr[idx])\n",
    "            print(f\"Sample {i+1} (index {idx}): One-hot: [{one_hot_str}], Mechanism Label: M{label+1:02d}\")\n",
    "    else:\n",
    "        print(\"Format: NOT one-hot encoding\")\n",
    "    print()\n",
    "    print('-'*80)\n",
    "\n",
    "print_label_info(y_train, \"y_train\")\n",
    "print_label_info(y_val, \"y_val\")\n",
    "print_label_info(y_test, \"y_test\")\n",
    "\n",
    "print(\"The label for each sample is the corresponding mechanism used to generate its kinetic data, encoded as a one-hot vector as follows:\\n\")\n",
    "for i in range(20):\n",
    "    vec = ['0']*20\n",
    "    vec[i] = '1'\n",
    "    print(f\"M{i+1:>2}: ({', '.join(vec)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c0a63-6709-4171-abc2-73eebf5dee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_classifier(input1_shape, input2_shape, output_shape, dropout_rate=0.3):\n",
    "    initial_concentrations = Input(shape=input1_shape)\n",
    "    kinetics = Input(shape=(None, input2_shape[-1]))\n",
    "\n",
    "    c = Dense(200, activation='relu')(initial_concentrations)\n",
    "    c = Dropout(dropout_rate)(c)\n",
    "\n",
    "    k = LSTM(200, return_sequences=True, dropout=dropout_rate)(kinetics)\n",
    "    k = LSTM(200, dropout=dropout_rate)(k)\n",
    "    k = Dropout(dropout_rate)(k)\n",
    "\n",
    "    combined = concatenate([c, k])\n",
    "    pred = Dense(200, activation='relu', kernel_initializer='he_uniform')(combined)\n",
    "    pred = Dropout(dropout_rate)(pred)\n",
    "    pred = Dense(output_shape[1], activation='softmax', name='Dense_5')(pred)\n",
    "\n",
    "    model = Model(inputs=[initial_concentrations, kinetics], outputs=pred)\n",
    "    return model\n",
    "\n",
    "class KineticsBatchGenerator(Sequence):\n",
    "    def __init__(self, X, y, tps, error_range, seed_value=1, batch_size=1024, shuffle=True):\n",
    "        self.x1, self.x2 = X\n",
    "        self.y = y\n",
    "        self.tps = tps\n",
    "        self.error_range = error_range\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.total_batches = self.__len__()\n",
    "        self.n_runs = 4\n",
    "        self.species = [1, 2]\n",
    "        self.error_species = len(self.species)\n",
    "        self.error_dict = {key: [] for key in error_range}\n",
    "        np.random.seed(seed_value)\n",
    "        self.randomstate = np.random.default_rng(seed_value)\n",
    "        self.on_epoch_end()\n",
    "        examples, timepoints, curves = self.x2.shape\n",
    "        columns = curves // self.n_runs\n",
    "        self.index_species = []\n",
    "        for i in range(self.n_runs):\n",
    "            t_species = [a + (i * columns) for a in self.species]\n",
    "            self.index_species = self.index_species + t_species\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__data_generation(index)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.y))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        for error in self.error_range:\n",
    "            if error != 0:\n",
    "                preu = np.array([gauss(0, error / 100) for _ in range(self.batch_size * np.max(self.tps) * self.error_species * self.n_runs)])\n",
    "                self.error_dict[error] = np.reshape(preu, (self.batch_size, np.max(self.tps), self.error_species * self.n_runs))\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        x1 = self.x1[self.batch_size * index: self.batch_size * (index + 1)]\n",
    "        x2 = self.introduce_error(self.trim_x2(self.x2[self.batch_size * index: self.batch_size * (index + 1)], self.tps, index))\n",
    "        y = self.y[self.batch_size * index: self.batch_size * (index + 1)]\n",
    "        x = [x1, x2]\n",
    "        return x, y\n",
    "\n",
    "    def trim_x2(self, original_x2, tps, index):\n",
    "        original_tps = original_x2.shape[1]\n",
    "        n_tps = self.randomstate.choice(tps)\n",
    "        idx = np.sort(np.append(self.randomstate.choice(original_tps - 1, n_tps, replace=False) + 1, [0]))\n",
    "        return original_x2[:, idx].copy()\n",
    "\n",
    "    def introduce_error(self, x2):\n",
    "        examples, timepoints, curves = x2.shape\n",
    "        error = self.randomstate.choice(self.error_range)\n",
    "        if error != 0:\n",
    "            x2[:, 1:, self.index_species] = x2[:, 1:, self.index_species] + self.error_dict[error][:, 0:timepoints - 1]\n",
    "        return x2\n",
    "\n",
    "def get_top_mechanism_indices(predictions):\n",
    "    threshold = 0.99\n",
    "    index = np.argsort(predictions)\n",
    "    prob = 0\n",
    "    grouping = []\n",
    "    for j in index[::-1]:\n",
    "        prob += predictions[j]\n",
    "        grouping.append(j)\n",
    "        if prob >= threshold:\n",
    "            break\n",
    "    return grouping\n",
    "\n",
    "def mechanism_indices_to_names(indices):\n",
    "    mechanism_list = ['M' + str(i) for i in range(1, 21)]\n",
    "    grouping_names = [mechanism_list[i] for i in indices]\n",
    "    return grouping_names\n",
    "\n",
    "def plot_kinetic_profiles_and_mechanisms(x_list, mechanism_names, columns=3, s=30, A0=1, show=True, overlay=False, labels=None):\n",
    "    x1, x2 = x_list\n",
    "    rows = len(x1)\n",
    "    curves = x2.shape[2] // columns\n",
    "    if not overlay:\n",
    "        plt.figure(figsize=(4 * columns, 7 * rows))\n",
    "    i = 1\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            plt.subplot(2, columns, i)\n",
    "            if not overlay:\n",
    "                if column == 3:\n",
    "                    title = r'[Cat]$_{0}$ = ' + str(round(x1[row, column], 5))\n",
    "                else:\n",
    "                    percentage = ' (' + str(round(x1[row, column] / A0 * 100, 3)) + ' mol%)'\n",
    "                    title = r'[Cat]$_{0}$ = ' + str(round(x1[row, column], 5)) + percentage\n",
    "                plt.title(title)\n",
    "            if type(labels) == list:\n",
    "                label = labels\n",
    "            else:\n",
    "                label = None\n",
    "            for j in range(1, curves):\n",
    "                if type(labels) == list:\n",
    "                    label = labels[j - 1]\n",
    "                else:\n",
    "                    label = None\n",
    "                plt.scatter(x2[row, :, column * curves], x2[row, :, (j) + column * curves], s, label=label)\n",
    "            if not overlay:\n",
    "                plt.xlim(left=0, right=max(x2[row, :, column * curves]) * 1.1)\n",
    "                plt.ylim(bottom=0)\n",
    "            i += 1\n",
    "    plt.tight_layout()\n",
    "    mechs = []\n",
    "    for i, name in enumerate(mechanism_names):\n",
    "        mechs.append(img.imread('Images/' + name + '.jpg'))\n",
    "    length = len(mechs)\n",
    "    for i, mech in enumerate(mechs):\n",
    "        plt.subplot(2, length, length + i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(mech)\n",
    "\n",
    "MODEL_COLUMN_MAP = {\n",
    "    'M1_20_model_bayes': ['Time', 'S', 'P', 'catT']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aad58-bdc2-479b-b54e-91222e65f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mechanism_model(\n",
    "    tps = [3,4,5,6,7,8,9,10,15,20],\n",
    "    error_range = [0,0.5,1,2],\n",
    "    epochs = 3000,\n",
    "    start = 0,\n",
    "    batch_size_training = 2048\n",
    "):\n",
    "    x1_train, x2_train, y_train, x1_val, x2_val, y_val, x1_test, x2_test, y_test = load_datasets()\n",
    "\n",
    "    model = build_lstm_classifier(\n",
    "        input1_shape = x1_train.shape[1:],\n",
    "        input2_shape = x2_train.shape[1:],\n",
    "        output_shape = y_train.shape  \n",
    "    )\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        patience=300,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"trained_model/best_model_weights\",\n",
    "        monitor='val_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        KineticsBatchGenerator([x1_train, x2_train], y_train, tps, error_range, batch_size=batch_size_training),\n",
    "        validation_data=KineticsBatchGenerator([x1_val, x2_val], y_val, tps, error_range, batch_size=25),\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        epochs=start+epochs,\n",
    "        initial_epoch=start\n",
    "    )\n",
    "\n",
    "    model.save('trained_model/M1_20_model_bayes')\n",
    "    print('Training completed. Optimal weights saved to: trained_model/best_model_weights')\n",
    "    print('Trained model saved to: trained_model/M1_20_model_bayes')\n",
    "    return model\n",
    "\n",
    "model = train_mechanism_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fe2c4-693e-48d1-b5ad-d9f9b94fb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, x_inputs, n_mc=100):\n",
    "    preds = [model(x_inputs, training=True).numpy() for _ in range(n_mc)]\n",
    "    all_probs = np.stack(preds, axis=0)\n",
    "    mean_probs = all_probs.mean(axis=0)\n",
    "    std_probs = all_probs.std(axis=0)\n",
    "    return mean_probs, std_probs, all_probs\n",
    "\n",
    "def credible_set(probs, threshold=0.95):\n",
    "    idx = np.argsort(probs)[::-1]\n",
    "    cumsum = np.cumsum(probs[idx])\n",
    "    k = np.searchsorted(cumsum, threshold) + 1\n",
    "    return idx[:k]\n",
    "\n",
    "def mechanism_indices_to_names(indices):\n",
    "    mechanism_list = ['M' + str(i) for i in range(1, 21)]\n",
    "    return [mechanism_list[i] for i in indices]\n",
    "\n",
    "model = load_model('trained_model/M1_20_model_bayes', compile=False)\n",
    "model.load_weights('trained_model/best_model_weights')\n",
    "\n",
    "mean_probs, std_probs, all_probs = mc_dropout_predict(model, [x1_test, x2_test], n_mc=100)\n",
    "entropies = entropy(mean_probs.T).T\n",
    "credible_sets = [credible_set(p, 0.95) for p in mean_probs]\n",
    "\n",
    "np.savez('bayesian_results_mc_dropout.npz',\n",
    "         mean_probs=mean_probs,\n",
    "         std_probs=std_probs,\n",
    "         entropies=entropies,\n",
    "         credible_sets=credible_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi-env]",
   "language": "python",
   "name": "conda-env-sbi-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
